{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BentoML Example\n",
    "# Titanic Survival Prediction with XGBoost\n",
    "\n",
    "This is a BentoML Demo Project demonstrating how to package and serve XGBoost model for production using BentoML.\n",
    "\n",
    "[BentoML](http://bentoml.ai) is an open source platform for machine learning model serving and deployment.\n",
    "\n",
    "Let's get started!\n",
    "![Impression](https://www.google-analytics.com/collect?v=1&tid=UA-112879361-3&cid=555&t=event&ec=xgboost&ea=xgboost-tiantic-survival-prediction&dt=xgboost-tiantic-survival-prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bentoml\n",
    "!pip install xgboost numpy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import bentoml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataset\n",
    "download dataset from https://www.kaggle.com/c/titanic/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: data: File exists\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 60302  100 60302    0     0   217k      0 --:--:-- --:--:-- --:--:--  216k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 28210  100 28210    0     0   133k      0 --:--:-- --:--:-- --:--:--  133k\n"
     ]
    }
   ],
   "source": [
    "!mkdir data\n",
    "!curl https://raw.githubusercontent.com/agconti/kaggle-titanic/master/data/train.csv -o ./data/train.csv\n",
    "!curl https://raw.githubusercontent.com/agconti/kaggle-titanic/master/data/test.csv -o ./data/test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./data/train.csv\")\n",
    "test  = pd.read_csv(\"./data/test.csv\")\n",
    "X_y_train = xgb.DMatrix(data=train[['Pclass', 'Age', 'Fare', 'SibSp', 'Parch']], label= train['Survived'])\n",
    "X_test    = xgb.DMatrix(data=test[['Pclass', 'Age', 'Fare', 'SibSp', 'Parch']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass   Age     Fare  SibSp  Parch  Survived\n",
       "0       3  22.0   7.2500      1      0         0\n",
       "1       1  38.0  71.2833      1      0         1\n",
       "2       3  26.0   7.9250      0      0         1\n",
       "3       1  35.0  53.1000      1      0         1\n",
       "4       3  35.0   8.0500      0      0         0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[['Pclass', 'Age', 'Fare', 'SibSp', 'Parch', 'Survived']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "          'base_score': np.mean(train['Survived']),\n",
    "          'eta':  0.1,\n",
    "          'max_depth': 3,\n",
    "          'gamma' :3,\n",
    "          'objective'   :'reg:linear',\n",
    "          'eval_metric' :'mae'\n",
    "         }\n",
    "model = xgb.train(params=params, \n",
    "                  dtrain=X_y_train, \n",
    "                  num_boost_round=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.341580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>46.0</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.413966</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Pclass   Age     Fare  SibSp  Parch      pred\n",
       "10       3   NaN   7.8958      0      0  0.341580\n",
       "11       1  46.0  26.0000      0      0  0.413966"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test =  model.predict(X_test)\n",
    "test['pred'] = y_test\n",
    "test[['Pclass', 'Age', 'Fare', 'SibSp', 'Parch','pred']].iloc[10:].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create BentoService for model serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting xgboost_titanic_bento_service.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile xgboost_titanic_bento_service.py\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import bentoml\n",
    "from bentoml.artifact import XgboostModelArtifact\n",
    "from bentoml.handlers import DataframeHandler\n",
    "\n",
    "@bentoml.artifacts([XgboostModelArtifact('model')])\n",
    "@bentoml.env(pip_dependencies=['xgboost'])\n",
    "class TitanicSurvivalPredictionService(bentoml.BentoService):\n",
    "    \n",
    "    @bentoml.api(DataframeHandler)\n",
    "    def predict(self, df):\n",
    "        data = xgb.DMatrix(data=df[['Pclass', 'Age', 'Fare', 'SibSp', 'Parch']])\n",
    "        return self.artifacts.model.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save BentoML service archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-12-11 17:47:09,677] WARNING - BentoML local changes detected - Local BentoML repository including all code changes will be bundled together with the BentoService bundle. When used with docker, the base docker image will be default to same version as last PyPI release at version: 0.5.3. You can also force bentoml to use a specific version for deploying your BentoService bundle, by setting the config 'core/bentoml_deploy_version' to a pinned version or your custom BentoML on github, e.g.:'bentoml_deploy_version = git+https://github.com/{username}/bentoml.git@{branch}'\n",
      "True\n",
      "[2019-12-11 17:47:09,679] WARNING - BentoML local changes detected - Local BentoML repository including all code changes will be bundled together with the BentoService bundle. When used with docker, the base docker image will be default to same version as last PyPI release at version: 0.5.3. You can also force bentoml to use a specific version for deploying your BentoService bundle, by setting the config 'core/bentoml_deploy_version' to a pinned version or your custom BentoML on github, e.g.:'bentoml_deploy_version = git+https://github.com/{username}/bentoml.git@{branch}'\n",
      "[2019-12-11 17:47:09,692] WARNING - BentoML local changes detected - Local BentoML repository including all code changes will be bundled together with the BentoService bundle. When used with docker, the base docker image will be default to same version as last PyPI release at version: 0.5.3. You can also force bentoml to use a specific version for deploying your BentoService bundle, by setting the config 'core/bentoml_deploy_version' to a pinned version or your custom BentoML on github, e.g.:'bentoml_deploy_version = git+https://github.com/{username}/bentoml.git@{branch}'\n",
      "[2019-12-11 17:47:34,508] WARNING - BentoML local changes detected - Local BentoML repository including all code changes will be bundled together with the BentoService bundle. When used with docker, the base docker image will be default to same version as last PyPI release at version: 0.5.3. You can also force bentoml to use a specific version for deploying your BentoService bundle, by setting the config 'core/bentoml_deploy_version' to a pinned version or your custom BentoML on github, e.g.:'bentoml_deploy_version = git+https://github.com/{username}/bentoml.git@{branch}'\n",
      "running sdist\n",
      "running egg_info\n",
      "writing BentoML.egg-info/PKG-INFO\n",
      "writing dependency_links to BentoML.egg-info/dependency_links.txt\n",
      "writing entry points to BentoML.egg-info/entry_points.txt\n",
      "writing requirements to BentoML.egg-info/requires.txt\n",
      "writing top-level names to BentoML.egg-info/top_level.txt\n",
      "reading manifest file 'BentoML.egg-info/SOURCES.txt'\n",
      "reading manifest template 'MANIFEST.in'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "no previously-included directories found matching 'examples'\n",
      "no previously-included directories found matching 'tests'\n",
      "no previously-included directories found matching 'docs'\n",
      "warning: no previously-included files matching '*~' found anywhere in distribution\n",
      "warning: no previously-included files matching '*.pyo' found anywhere in distribution\n",
      "warning: no previously-included files matching '.git' found anywhere in distribution\n",
      "warning: no previously-included files matching '.ipynb_checkpoints' found anywhere in distribution\n",
      "warning: no previously-included files matching '__pycache__' found anywhere in distribution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing manifest file 'BentoML.egg-info/SOURCES.txt'\n",
      "running check\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: check: missing meta-data: if 'author' supplied, 'author_email' must be supplied too\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating BentoML-0.5.3+22.g10ffa8c\n",
      "creating BentoML-0.5.3+22.g10ffa8c/BentoML.egg-info\n",
      "creating BentoML-0.5.3+22.g10ffa8c/bentoml\n",
      "creating BentoML-0.5.3+22.g10ffa8c/bentoml/artifact\n",
      "creating BentoML-0.5.3+22.g10ffa8c/bentoml/bundler\n",
      "creating BentoML-0.5.3+22.g10ffa8c/bentoml/cli\n",
      "creating BentoML-0.5.3+22.g10ffa8c/bentoml/clipper\n",
      "creating BentoML-0.5.3+22.g10ffa8c/bentoml/configuration\n",
      "creating BentoML-0.5.3+22.g10ffa8c/bentoml/deployment\n",
      "creating BentoML-0.5.3+22.g10ffa8c/bentoml/deployment/aws_lambda\n",
      "creating BentoML-0.5.3+22.g10ffa8c/bentoml/deployment/sagemaker\n",
      "creating BentoML-0.5.3+22.g10ffa8c/bentoml/deployment/serverless\n",
      "creating BentoML-0.5.3+22.g10ffa8c/bentoml/handlers\n",
      "creating BentoML-0.5.3+22.g10ffa8c/bentoml/migrations\n",
      "creating BentoML-0.5.3+22.g10ffa8c/bentoml/migrations/versions\n",
      "creating BentoML-0.5.3+22.g10ffa8c/bentoml/proto\n",
      "creating BentoML-0.5.3+22.g10ffa8c/bentoml/repository\n",
      "creating BentoML-0.5.3+22.g10ffa8c/bentoml/server\n",
      "creating BentoML-0.5.3+22.g10ffa8c/bentoml/server/static\n",
      "creating BentoML-0.5.3+22.g10ffa8c/bentoml/utils\n",
      "creating BentoML-0.5.3+22.g10ffa8c/bentoml/utils/validator\n",
      "creating BentoML-0.5.3+22.g10ffa8c/bentoml/yatai\n",
      "copying files to BentoML-0.5.3+22.g10ffa8c...\n",
      "copying LICENSE -> BentoML-0.5.3+22.g10ffa8c\n",
      "copying MANIFEST.in -> BentoML-0.5.3+22.g10ffa8c\n",
      "copying README.md -> BentoML-0.5.3+22.g10ffa8c\n",
      "copying setup.cfg -> BentoML-0.5.3+22.g10ffa8c\n",
      "copying setup.py -> BentoML-0.5.3+22.g10ffa8c\n",
      "copying versioneer.py -> BentoML-0.5.3+22.g10ffa8c\n",
      "copying BentoML.egg-info/PKG-INFO -> BentoML-0.5.3+22.g10ffa8c/BentoML.egg-info\n",
      "copying BentoML.egg-info/SOURCES.txt -> BentoML-0.5.3+22.g10ffa8c/BentoML.egg-info\n",
      "copying BentoML.egg-info/dependency_links.txt -> BentoML-0.5.3+22.g10ffa8c/BentoML.egg-info\n",
      "copying BentoML.egg-info/entry_points.txt -> BentoML-0.5.3+22.g10ffa8c/BentoML.egg-info\n",
      "copying BentoML.egg-info/requires.txt -> BentoML-0.5.3+22.g10ffa8c/BentoML.egg-info\n",
      "copying BentoML.egg-info/top_level.txt -> BentoML-0.5.3+22.g10ffa8c/BentoML.egg-info\n",
      "copying bentoml/__init__.py -> BentoML-0.5.3+22.g10ffa8c/bentoml\n",
      "copying bentoml/_version.py -> BentoML-0.5.3+22.g10ffa8c/bentoml\n",
      "copying bentoml/alembic.ini -> BentoML-0.5.3+22.g10ffa8c/bentoml\n",
      "copying bentoml/db.py -> BentoML-0.5.3+22.g10ffa8c/bentoml\n",
      "copying bentoml/exceptions.py -> BentoML-0.5.3+22.g10ffa8c/bentoml\n",
      "copying bentoml/service.py -> BentoML-0.5.3+22.g10ffa8c/bentoml\n",
      "copying bentoml/service_env.py -> BentoML-0.5.3+22.g10ffa8c/bentoml\n",
      "copying bentoml/artifact/__init__.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/artifact\n",
      "copying bentoml/artifact/artifact.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/artifact\n",
      "copying bentoml/artifact/fastai_model_artifact.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/artifact\n",
      "copying bentoml/artifact/h2o_model_artifact.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/artifact\n",
      "copying bentoml/artifact/keras_model_artifact.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/artifact\n",
      "copying bentoml/artifact/lightgbm_model_artifact.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/artifact\n",
      "copying bentoml/artifact/pickle_artifact.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/artifact\n",
      "copying bentoml/artifact/pytorch_model_artifact.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/artifact\n",
      "copying bentoml/artifact/sklearn_model_artifact.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/artifact\n",
      "copying bentoml/artifact/text_file_artifact.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/artifact\n",
      "copying bentoml/artifact/tf_savedmodel_artifact.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/artifact\n",
      "copying bentoml/artifact/xgboost_model_artifact.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/artifact\n",
      "copying bentoml/bundler/__init__.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/bundler\n",
      "copying bentoml/bundler/bundler.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/bundler\n",
      "copying bentoml/bundler/config.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/bundler\n",
      "copying bentoml/bundler/loader.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/bundler\n",
      "copying bentoml/bundler/py_module_utils.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/bundler\n",
      "copying bentoml/bundler/templates.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/bundler\n",
      "copying bentoml/bundler/utils.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/bundler\n",
      "copying bentoml/cli/__init__.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/cli\n",
      "copying bentoml/cli/click_utils.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/cli\n",
      "copying bentoml/cli/config.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/cli\n",
      "copying bentoml/cli/deployment.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/cli\n",
      "copying bentoml/cli/utils.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/cli\n",
      "copying bentoml/clipper/__init__.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/clipper\n",
      "copying bentoml/configuration/__init__.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/configuration\n",
      "copying bentoml/configuration/configparser.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/configuration\n",
      "copying bentoml/configuration/default_bentoml.cfg -> BentoML-0.5.3+22.g10ffa8c/bentoml/configuration\n",
      "copying bentoml/deployment/__init__.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/deployment\n",
      "copying bentoml/deployment/operator.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/deployment\n",
      "copying bentoml/deployment/store.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/deployment\n",
      "copying bentoml/deployment/utils.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/deployment\n",
      "copying bentoml/deployment/aws_lambda/__init__.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/deployment/aws_lambda\n",
      "copying bentoml/deployment/aws_lambda/lambda_app.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/deployment/aws_lambda\n",
      "copying bentoml/deployment/aws_lambda/unzip_requirements.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/deployment/aws_lambda\n",
      "copying bentoml/deployment/aws_lambda/utils.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/deployment/aws_lambda\n",
      "copying bentoml/deployment/sagemaker/__init__.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/deployment/sagemaker\n",
      "copying bentoml/deployment/sagemaker/templates.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/deployment/sagemaker\n",
      "copying bentoml/deployment/serverless/__init__.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/deployment/serverless\n",
      "copying bentoml/deployment/serverless/aws_lambda.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/deployment/serverless\n",
      "copying bentoml/deployment/serverless/gcp_function.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/deployment/serverless\n",
      "copying bentoml/deployment/serverless/serverless_utils.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/deployment/serverless\n",
      "copying bentoml/handlers/__init__.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/handlers\n",
      "copying bentoml/handlers/base_handlers.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/handlers\n",
      "copying bentoml/handlers/clipper_handler.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/handlers\n",
      "copying bentoml/handlers/dataframe_handler.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/handlers\n",
      "copying bentoml/handlers/fastai_image_handler.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/handlers\n",
      "copying bentoml/handlers/image_handler.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/handlers\n",
      "copying bentoml/handlers/json_handler.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/handlers\n",
      "copying bentoml/handlers/pytorch_tensor_handler.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/handlers\n",
      "copying bentoml/handlers/tensorflow_tensor_handler.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/handlers\n",
      "copying bentoml/migrations/README -> BentoML-0.5.3+22.g10ffa8c/bentoml/migrations\n",
      "copying bentoml/migrations/env.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/migrations\n",
      "copying bentoml/migrations/script.py.mako -> BentoML-0.5.3+22.g10ffa8c/bentoml/migrations\n",
      "copying bentoml/migrations/versions/a6b00ae45279_add_last_updated_at_for_deployments.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/migrations/versions\n",
      "copying bentoml/proto/__init__.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/proto\n",
      "copying bentoml/proto/deployment_pb2.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/proto\n",
      "copying bentoml/proto/repository_pb2.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/proto\n",
      "copying bentoml/proto/status_pb2.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/proto\n",
      "copying bentoml/proto/yatai_service_pb2.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/proto\n",
      "copying bentoml/proto/yatai_service_pb2_grpc.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/proto\n",
      "copying bentoml/repository/__init__.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/repository\n",
      "copying bentoml/repository/metadata_store.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/repository\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copying bentoml/server/__init__.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/server\n",
      "copying bentoml/server/bento_api_server.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/server\n",
      "copying bentoml/server/bento_sagemaker_server.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/server\n",
      "copying bentoml/server/gunicorn_config.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/server\n",
      "copying bentoml/server/gunicorn_server.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/server\n",
      "copying bentoml/server/utils.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/server\n",
      "copying bentoml/server/static/swagger-ui-bundle.js -> BentoML-0.5.3+22.g10ffa8c/bentoml/server/static\n",
      "copying bentoml/server/static/swagger-ui.css -> BentoML-0.5.3+22.g10ffa8c/bentoml/server/static\n",
      "copying bentoml/utils/__init__.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/utils\n",
      "copying bentoml/utils/cloudpickle.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/utils\n",
      "copying bentoml/utils/hybirdmethod.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/utils\n",
      "copying bentoml/utils/log.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/utils\n",
      "copying bentoml/utils/s3.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/utils\n",
      "copying bentoml/utils/tempdir.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/utils\n",
      "copying bentoml/utils/usage_stats.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/utils\n",
      "copying bentoml/utils/whichcraft.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/utils\n",
      "copying bentoml/utils/validator/__init__.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/utils/validator\n",
      "copying bentoml/yatai/__init__.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/yatai\n",
      "copying bentoml/yatai/deployment_utils.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/yatai\n",
      "copying bentoml/yatai/python_api.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/yatai\n",
      "copying bentoml/yatai/status.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/yatai\n",
      "copying bentoml/yatai/yatai_service_impl.py -> BentoML-0.5.3+22.g10ffa8c/bentoml/yatai\n",
      "Writing BentoML-0.5.3+22.g10ffa8c/setup.cfg\n",
      "UPDATING BentoML-0.5.3+22.g10ffa8c/bentoml/_version.py\n",
      "set BentoML-0.5.3+22.g10ffa8c/bentoml/_version.py to '0.5.3+22.g10ffa8c'\n",
      "Creating tar archive\n",
      "removing 'BentoML-0.5.3+22.g10ffa8c' (and everything under it)\n",
      "[2019-12-11 17:47:36,534] INFO - BentoService bundle 'TitanicSurvivalPredictionService:20191211174709_CCB40F' created at: /private/var/folders/kn/xnc9k74x03567n1mx2tfqnpr0000gn/T/bentoml-temp-s963i88c\n",
      "[2019-12-11 17:47:36,536] WARNING - BentoML local changes detected - Local BentoML repository including all code changes will be bundled together with the BentoService bundle. When used with docker, the base docker image will be default to same version as last PyPI release at version: 0.5.3. You can also force bentoml to use a specific version for deploying your BentoService bundle, by setting the config 'core/bentoml_deploy_version' to a pinned version or your custom BentoML on github, e.g.:'bentoml_deploy_version = git+https://github.com/{username}/bentoml.git@{branch}'\n",
      "[2019-12-11 17:47:36,550] WARNING - Saved BentoService bundle version mismatch: loading BentoServie bundle create with BentoML version 0.5.3,  but loading from BentoML version 0.5.3+22.g10ffa8c\n",
      "[2019-12-11 17:47:36,771] INFO - BentoService bundle 'TitanicSurvivalPredictionService:20191211174709_CCB40F' created at: /Users/bozhaoyu/bentoml/repository/TitanicSurvivalPredictionService/20191211174709_CCB40F\n"
     ]
    }
   ],
   "source": [
    "# 1) import the custom BentoService defined above\n",
    "from xgboost_titanic_bento_service import TitanicSurvivalPredictionService\n",
    "\n",
    "# 2) `pack` it with required artifacts\n",
    "bento_service = TitanicSurvivalPredictionService()\n",
    "bento_service.pack('model', model)\n",
    "\n",
    "# 3) save your BentoSerivce\n",
    "saved_path = bento_service.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load saved BentoService for serving\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-12-11 17:48:15,614] WARNING - BentoML local changes detected - Local BentoML repository including all code changes will be bundled together with the BentoService bundle. When used with docker, the base docker image will be default to same version as last PyPI release at version: 0.5.3. You can also force bentoml to use a specific version for deploying your BentoService bundle, by setting the config 'core/bentoml_deploy_version' to a pinned version or your custom BentoML on github, e.g.:'bentoml_deploy_version = git+https://github.com/{username}/bentoml.git@{branch}'\n",
      "[2019-12-11 17:48:15,629] WARNING - Saved BentoService bundle version mismatch: loading BentoServie bundle create with BentoML version 0.5.3,  but loading from BentoML version 0.5.3+22.g10ffa8c\n",
      "[2019-12-11 17:48:15,632] WARNING - Module `xgboost_titanic_bento_service` already loaded, using existing imported module.\n",
      "[2019-12-11 17:48:15,638] WARNING - BentoML local changes detected - Local BentoML repository including all code changes will be bundled together with the BentoService bundle. When used with docker, the base docker image will be default to same version as last PyPI release at version: 0.5.3. You can also force bentoml to use a specific version for deploying your BentoService bundle, by setting the config 'core/bentoml_deploy_version' to a pinned version or your custom BentoML on github, e.g.:'bentoml_deploy_version = git+https://github.com/{username}/bentoml.git@{branch}'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.341580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>46.0</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.413966</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Pclass   Age     Fare  SibSp  Parch      pred\n",
       "10       3   NaN   7.8958      0      0  0.341580\n",
       "11       1  46.0  26.0000      0      0  0.413966"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bentoml\n",
    "\n",
    "bento_model = bentoml.load(saved_path)\n",
    "\n",
    "result = bento_model.predict(test)\n",
    "test['pred'] = result\n",
    "test[['Pclass', 'Age', 'Fare', 'SibSp', 'Parch','pred']].iloc[10:].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Serving via REST API\n",
    "\n",
    "In your termnial, run the following command to start the REST API server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:20:10] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      " * Serving Flask app \"TitanicSurvivalPredictionService\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n",
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [19/Sep/2019 14:20:14] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [19/Sep/2019 14:20:16] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!bentoml serve {saved_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy following command to make a curl request to Rest API server\n",
    "\n",
    "```bash\n",
    "curl -i \\\n",
    "--header \"Content-Type: application/json\" \\\n",
    "--request POST \\\n",
    "--data '[{\"Pclass\": 1, \"Age\": 30, \"Fare\": 200, \"SibSp\": 1, \"Parch\": 0}]' \\\n",
    "localhost:5000/predict\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy it as API endpoint on AWS Lambda\n",
    "\n",
    "In order to run this as AWS Lambda function, make sure to configure your AWS credentials via either aws configure command or setting the environment variables below:\n",
    "```\n",
    "%env AWS_ACCESS_KEY_ID=\n",
    "%env AWS_SECRET_ACCESS_KEY=\n",
    "%env AWS_DEFAULT_REGION=\n",
    "```\n",
    "Make sure you have AWS SAM CLI installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env AWS_ACCESS_KEY_ID=\n",
    "%env AWS_SECRET_ACCESS_KEY=\n",
    "%env AWS_DEFAULT_REGION="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have AWS SAM CLI installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U aws-sam-cli==0.33.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-12-12 15:38:26,831] INFO - Building lambda project\n",
      "[2019-12-12 15:41:29,703] INFO - Packaging AWS Lambda project at /private/var/folders/kn/xnc9k74x03567n1mx2tfqnpr0000gn/T/bentoml-temp-_2fjzrcs ...\n",
      "[2019-12-12 15:41:54,482] INFO - Deploying lambda project\n",
      "[2019-12-12 15:43:44,620] INFO - ApplyDeployment (titanic-prediction, namespace bobo) succeeded\n",
      "\u001b[32mSuccessfully created deployment titanic-prediction\u001b[0m\n",
      "\u001b[39m{\n",
      "  \"namespace\": \"bobo\",\n",
      "  \"name\": \"titanic-prediction\",\n",
      "  \"spec\": {\n",
      "    \"bentoName\": \"TitanicSurvivalPredictionService\",\n",
      "    \"bentoVersion\": \"20191211174709_CCB40F\",\n",
      "    \"operator\": \"AWS_LAMBDA\",\n",
      "    \"awsLambdaOperatorConfig\": {\n",
      "      \"region\": \"us-west-2\",\n",
      "      \"memorySize\": 1024,\n",
      "      \"timeout\": 3\n",
      "    }\n",
      "  },\n",
      "  \"state\": {\n",
      "    \"state\": \"RUNNING\",\n",
      "    \"infoJson\": {\n",
      "      \"endpoints\": [\n",
      "        \"https://u7nerrir6a.execute-api.us-west-2.amazonaws.com/Prod/predict\"\n",
      "      ],\n",
      "      \"s3_bucket\": \"btml-bobo-titanic-prediction-c3da9d\"\n",
      "    },\n",
      "    \"timestamp\": \"2019-12-12T23:43:44.799944Z\"\n",
      "  },\n",
      "  \"createdAt\": \"2019-12-12T23:38:22.515572Z\",\n",
      "  \"lastUpdatedAt\": \"2019-12-12T23:38:22.515613Z\"\n",
      "}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!bentoml deployment create titanic-prediction \\\n",
    "    --bento=TitanicSurvivalPredictionService:{bento_service.version} \\\n",
    "    --platform=aws-lambda \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[39m{\r\n",
      "  \"namespace\": \"bobo\",\r\n",
      "  \"name\": \"titanic-prediction\",\r\n",
      "  \"spec\": {\r\n",
      "    \"bentoName\": \"TitanicSurvivalPredictionService\",\r\n",
      "    \"bentoVersion\": \"20191211174709_CCB40F\",\r\n",
      "    \"operator\": \"AWS_LAMBDA\",\r\n",
      "    \"awsLambdaOperatorConfig\": {\r\n",
      "      \"region\": \"us-west-2\",\r\n",
      "      \"memorySize\": 1024,\r\n",
      "      \"timeout\": 3\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"state\": {\r\n",
      "    \"state\": \"RUNNING\",\r\n",
      "    \"infoJson\": {\r\n",
      "      \"endpoints\": [\r\n",
      "        \"https://eg3tvb519i.execute-api.us-west-2.amazonaws.com/Prod/predict\"\r\n",
      "      ],\r\n",
      "      \"s3_bucket\": \"btml-bobo-titanic-prediction-89a148\"\r\n",
      "    },\r\n",
      "    \"timestamp\": \"2019-12-12T01:54:02.842272Z\"\r\n",
      "  },\r\n",
      "  \"createdAt\": \"2019-12-12T01:49:36.076229Z\",\r\n",
      "  \"lastUpdatedAt\": \"2019-12-12T01:49:36.076270Z\"\r\n",
      "}\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!bentoml deployment describe titanic-prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To send request to your AWS Lambda deployment, grab the endpoint URL from the json output above:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP/1.1 200 OK\r",
      "\r\n",
      "Content-Type: application/json\r",
      "\r\n",
      "Content-Length: 19\r",
      "\r\n",
      "Connection: keep-alive\r",
      "\r\n",
      "Date: Thu, 12 Dec 2019 23:44:21 GMT\r",
      "\r\n",
      "x-amzn-RequestId: dbfc9834-a06e-4413-8bfb-f1ba735152c5\r",
      "\r\n",
      "x-amz-apigw-id: EnWQBFvzvHcF6pg=\r",
      "\r\n",
      "X-Amzn-Trace-Id: Root=1-5df2d0cc-b3097ff8a631eec860c1e000;Sampled=0\r",
      "\r\n",
      "X-Cache: Miss from cloudfront\r",
      "\r\n",
      "Via: 1.1 05aec04162b0fed6e9762cd1edd66a72.cloudfront.net (CloudFront)\r",
      "\r\n",
      "X-Amz-Cf-Pop: SFO5-C1\r",
      "\r\n",
      "X-Amz-Cf-Id: DoRs0y5UT_2M0Yz5lT7A_yTDspatZCzUfxIYiXElunifCkgXE_Ob-g==\r",
      "\r\n",
      "\r",
      "\r\n",
      "[0.469721257686615]"
     ]
    }
   ],
   "source": [
    "!curl -i \\\n",
    "--header \"Content-Type: application/json\" \\\n",
    "--request POST \\\n",
    "--data '[{\"Pclass\": 1, \"Age\": 30, \"Fare\": 200, \"SibSp\": 1, \"Parch\": 0}]' \\\n",
    "https://u7nerrir6a.execute-api.us-west-2.amazonaws.com/Prod/predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mSuccessfully deleted deployment \"titanic-prediction\"\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!bentoml deployment delete titanic-prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
